{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import collections\n",
    "import json\n",
    "import re \n",
    "\n",
    "import modeling\n",
    "import six \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelTest(tf.test.TestCase):\n",
    "\n",
    "    class BertModelTester(object):\n",
    "\n",
    "        def __init__(self,\n",
    "                     parent,\n",
    "                     batch_size=13,\n",
    "                     seq_len=7,\n",
    "                     is_training=True,\n",
    "                     use_input_mask=True,\n",
    "                     use_token_type_ids=True,\n",
    "                    vocab_size = 99,\n",
    "                    hidden_size = 32,\n",
    "                    num_hidden_layers = 5,\n",
    "                    num_attention_heads = 4,\n",
    "                    intermediate_size = 37,\n",
    "                    hidden_act = \"gelu\",\n",
    "                    hidden_dropout_prob = 0.1,\n",
    "                    attention_probs_dropout_prob = 0.1,\n",
    "                    max_position_embeddings=512,\n",
    "                    type_vocab_size = 16,\n",
    "                    initializer_range = 0.02,\n",
    "                    scope = None\n",
    "\n",
    "                     ) -> None:\n",
    "            \n",
    "\n",
    "            self.parent = parent\n",
    "            self.batch_size = batch_size\n",
    "            self.seq_len = seq_len\n",
    "            self.is_trianing = is_training\n",
    "            self.use_input_mask = use_input_mask\n",
    "            self.use_token_type_ids = use_token_type_ids\n",
    "            self.vocab_size = vocab_size\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_hidden_layer = num_hidden_layers\n",
    "            self.num_attention_heads = num_attention_heads\n",
    "            self.intermediate_size = intermediate_size\n",
    "            self.hidden_act = hidden_act\n",
    "            self.hidden_dropout_prob = hidden_dropout_prob\n",
    "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "            self.max_position_embeddings = max_position_embeddings\n",
    "            self.type_vocab_size = type_vocab_size\n",
    "            self.initializer_range = initializer_range\n",
    "            self.scope = scope\n",
    "\n",
    "\n",
    "\n",
    "        def create_model(self):\n",
    "            input_ids = BertModelTest.ids_tensor([self.batch_size, self.seq_len],\n",
    "                                                 self.vocab_size)\n",
    "            \n",
    "            input_mask = None \n",
    "            if self.use_input_mask:\n",
    "                input_mask = BertModelTest.ids_tensor(\n",
    "                    [self.batch_size, self.seq_len], vocab_size = 2\n",
    "                )\n",
    "\n",
    "            token_type_ids = None \n",
    "            if self.use_token_type_ids:\n",
    "                token_type_ids = BertModelTest.ids_tensor(\n",
    "                    [self.batch_size, self.seq_len], self.type_vocab_size\n",
    "                )\n",
    "\n",
    "            config = modeling.BertConfig(\n",
    "                    vocab_size=self.vocab_size,\n",
    "                    hidden_size=self.hidden_size,\n",
    "                    num_hidden_layers=self.num_hidden_layers,\n",
    "                    num_attention_heads=self.num_attention_heads,\n",
    "                    intermediate_size=self.intermediate_size,\n",
    "                    hidden_act=self.hidden_act,\n",
    "                    hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "                    attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n",
    "                    max_position_embeddings=self.max_position_embeddings,\n",
    "                    type_vocab_size=self.type_vocab_size,\n",
    "                    initializer_range=self.initializer_range)\n",
    "            \n",
    "            model = modeling.BertModel(\n",
    "                config=config,\n",
    "                is_training=self.is_training,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                scope= self.scope\n",
    "            )\n",
    "\n",
    "            outputs = {\n",
    "                    \"embedding_output\": model.get_embedding_output(),\n",
    "                    \"sequence_output\": model.get_sequence_output(),\n",
    "                    \"pooled_output\": model.get_pooled_output(),\n",
    "                    \"all_encoder_layers\": model.get_all_encoder_layers(),\n",
    "            }\n",
    "            return outputs\n",
    "        \n",
    "\n",
    "        def check_output(self, result):\n",
    "            self.parent.assertAllEqual(\n",
    "                result[\"embedding_output\"].shape,\n",
    "                [self.batch_size,\n",
    "                 self.seq_len,\n",
    "                 self.hidden_size]\n",
    "            )\n",
    "\n",
    "            self.parent.assertAllEqual(\n",
    "                result[\"sequence_output\"].shape,\n",
    "                [self.batch_size, self.seq_len, self.hidden_size]\n",
    "            )\n",
    "\n",
    "            self.parent.assertAllEqual(result[\"pooled_output\"].shape,\n",
    "                                       [self.batch_size, self.hidden_act])\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL Flags parsing error: Unknown command line flag 'f'\n",
      "Pass --helpshort or --helpfull to see help on flags.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manjusha Kumari\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "import modeling\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class BertModelTest(tf.test.TestCase):\n",
    "\n",
    "  class BertModelTester(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 parent,\n",
    "                 batch_size=13,\n",
    "                 seq_length=7,\n",
    "                 is_training=True,\n",
    "                 use_input_mask=True,\n",
    "                 use_token_type_ids=True,\n",
    "                 vocab_size=99,\n",
    "                 hidden_size=32,\n",
    "                 num_hidden_layers=5,\n",
    "                 num_attention_heads=4,\n",
    "                 intermediate_size=37,\n",
    "                 hidden_act=\"gelu\",\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=16,\n",
    "                 initializer_range=0.02,\n",
    "                 scope=None):\n",
    "      self.parent = parent\n",
    "      self.batch_size = batch_size\n",
    "      self.seq_length = seq_length\n",
    "      self.is_training = is_training\n",
    "      self.use_input_mask = use_input_mask\n",
    "      self.use_token_type_ids = use_token_type_ids\n",
    "      self.vocab_size = vocab_size\n",
    "      self.hidden_size = hidden_size\n",
    "      self.num_hidden_layers = num_hidden_layers\n",
    "      self.num_attention_heads = num_attention_heads\n",
    "      self.intermediate_size = intermediate_size\n",
    "      self.hidden_act = hidden_act\n",
    "      self.hidden_dropout_prob = hidden_dropout_prob\n",
    "      self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "      self.max_position_embeddings = max_position_embeddings\n",
    "      self.type_vocab_size = type_vocab_size\n",
    "      self.initializer_range = initializer_range\n",
    "      self.scope = scope\n",
    "\n",
    "    def create_model(self):\n",
    "      input_ids = BertModelTest.ids_tensor([self.batch_size, self.seq_length],\n",
    "                                           self.vocab_size)\n",
    "\n",
    "      input_mask = None\n",
    "      if self.use_input_mask:\n",
    "        input_mask = BertModelTest.ids_tensor(\n",
    "            [self.batch_size, self.seq_length], vocab_size=2)\n",
    "\n",
    "      token_type_ids = None\n",
    "      if self.use_token_type_ids:\n",
    "        token_type_ids = BertModelTest.ids_tensor(\n",
    "            [self.batch_size, self.seq_length], self.type_vocab_size)\n",
    "\n",
    "      config = modeling.BertConfig(\n",
    "          vocab_size=self.vocab_size,\n",
    "          hidden_size=self.hidden_size,\n",
    "          num_hidden_layers=self.num_hidden_layers,\n",
    "          num_attention_heads=self.num_attention_heads,\n",
    "          intermediate_size=self.intermediate_size,\n",
    "          hidden_act=self.hidden_act,\n",
    "          hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "          attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n",
    "          max_position_embeddings=self.max_position_embeddings,\n",
    "          type_vocab_size=self.type_vocab_size,\n",
    "          initializer_range=self.initializer_range)\n",
    "\n",
    "      model = modeling.BertModel(\n",
    "          config=config,\n",
    "          is_training=self.is_training,\n",
    "          input_ids=input_ids,\n",
    "          input_mask=input_mask,\n",
    "          token_type_ids=token_type_ids,\n",
    "          scope=self.scope)\n",
    "\n",
    "      outputs = {\n",
    "          \"embedding_output\": model.get_embedding_output(),\n",
    "          \"sequence_output\": model.get_sequence_output(),\n",
    "          \"pooled_output\": model.get_pooled_output(),\n",
    "          \"all_encoder_layers\": model.get_all_encoder_layers(),\n",
    "      }\n",
    "      return outputs\n",
    "\n",
    "    def check_output(self, result):\n",
    "      self.parent.assertAllEqual(\n",
    "          result[\"embedding_output\"].shape,\n",
    "          [self.batch_size, self.seq_length, self.hidden_size])\n",
    "\n",
    "      self.parent.assertAllEqual(\n",
    "          result[\"sequence_output\"].shape,\n",
    "          [self.batch_size, self.seq_length, self.hidden_size])\n",
    "\n",
    "      self.parent.assertAllEqual(result[\"pooled_output\"].shape,\n",
    "                                 [self.batch_size, self.hidden_size])\n",
    "\n",
    "  def test_default(self):\n",
    "    self.run_tester(BertModelTest.BertModelTester(self))\n",
    "\n",
    "  def test_config_to_json_string(self):\n",
    "    config = modeling.BertConfig(vocab_size=99, hidden_size=37)\n",
    "    obj = json.loads(config.to_json_string())\n",
    "    self.assertEqual(obj[\"vocab_size\"], 99)\n",
    "    self.assertEqual(obj[\"hidden_size\"], 37)\n",
    "\n",
    "  def run_tester(self, tester):\n",
    "    with self.test_session() as sess:\n",
    "      ops = tester.create_model()\n",
    "      init_op = tf.group(tf.global_variables_initializer(),\n",
    "                         tf.local_variables_initializer())\n",
    "      sess.run(init_op)\n",
    "      output_result = sess.run(ops)\n",
    "      tester.check_output(output_result)\n",
    "\n",
    "      self.assert_all_tensors_reachable(sess, [init_op, ops])\n",
    "\n",
    "  @classmethod\n",
    "  def ids_tensor(cls, shape, vocab_size, rng=None, name=None):\n",
    "    \"\"\"Creates a random int32 tensor of the shape within the vocab size.\"\"\"\n",
    "    if rng is None:\n",
    "      rng = random.Random()\n",
    "\n",
    "    total_dims = 1\n",
    "    for dim in shape:\n",
    "      total_dims *= dim\n",
    "\n",
    "    values = []\n",
    "    for _ in range(total_dims):\n",
    "      values.append(rng.randint(0, vocab_size - 1))\n",
    "\n",
    "    return tf.constant(value=values, dtype=tf.int32, shape=shape, name=name)\n",
    "\n",
    "  def assert_all_tensors_reachable(self, sess, outputs):\n",
    "    \"\"\"Checks that all the tensors in the graph are reachable from outputs.\"\"\"\n",
    "    graph = sess.graph\n",
    "\n",
    "    ignore_strings = [\n",
    "        \"^.*/assert_less_equal/.*$\",\n",
    "        \"^.*/dilation_rate$\",\n",
    "        \"^.*/Tensordot/concat$\",\n",
    "        \"^.*/Tensordot/concat/axis$\",\n",
    "        \"^testing/.*$\",\n",
    "    ]\n",
    "\n",
    "    ignore_regexes = [re.compile(x) for x in ignore_strings]\n",
    "\n",
    "    unreachable = self.get_unreachable_ops(graph, outputs)\n",
    "    filtered_unreachable = []\n",
    "    for x in unreachable:\n",
    "      do_ignore = False\n",
    "      for r in ignore_regexes:\n",
    "        m = r.match(x.name)\n",
    "        if m is not None:\n",
    "          do_ignore = True\n",
    "      if do_ignore:\n",
    "        continue\n",
    "      filtered_unreachable.append(x)\n",
    "    unreachable = filtered_unreachable\n",
    "\n",
    "    self.assertEqual(\n",
    "        len(unreachable), 0, \"The following ops are unreachable: %s\" %\n",
    "        (\" \".join([x.name for x in unreachable])))\n",
    "\n",
    "  @classmethod\n",
    "  def get_unreachable_ops(cls, graph, outputs):\n",
    "    \"\"\"Finds all of the tensors in graph that are unreachable from outputs.\"\"\"\n",
    "    outputs = cls.flatten_recursive(outputs)\n",
    "    output_to_op = collections.defaultdict(list)\n",
    "    op_to_all = collections.defaultdict(list)\n",
    "    assign_out_to_in = collections.defaultdict(list)\n",
    "\n",
    "    for op in graph.get_operations():\n",
    "      for x in op.inputs:\n",
    "        op_to_all[op.name].append(x.name)\n",
    "      for y in op.outputs:\n",
    "        output_to_op[y.name].append(op.name)\n",
    "        op_to_all[op.name].append(y.name)\n",
    "      if str(op.type) == \"Assign\":\n",
    "        for y in op.outputs:\n",
    "          for x in op.inputs:\n",
    "            assign_out_to_in[y.name].append(x.name)\n",
    "\n",
    "    assign_groups = collections.defaultdict(list)\n",
    "    for out_name in assign_out_to_in.keys():\n",
    "      name_group = assign_out_to_in[out_name]\n",
    "      for n1 in name_group:\n",
    "        assign_groups[n1].append(out_name)\n",
    "        for n2 in name_group:\n",
    "          if n1 != n2:\n",
    "            assign_groups[n1].append(n2)\n",
    "\n",
    "    seen_tensors = {}\n",
    "    stack = [x.name for x in outputs]\n",
    "    while stack:\n",
    "      name = stack.pop()\n",
    "      if name in seen_tensors:\n",
    "        continue\n",
    "      seen_tensors[name] = True\n",
    "\n",
    "      if name in output_to_op:\n",
    "        for op_name in output_to_op[name]:\n",
    "          if op_name in op_to_all:\n",
    "            for input_name in op_to_all[op_name]:\n",
    "              if input_name not in stack:\n",
    "                stack.append(input_name)\n",
    "\n",
    "      expanded_names = []\n",
    "      if name in assign_groups:\n",
    "        for assign_name in assign_groups[name]:\n",
    "          expanded_names.append(assign_name)\n",
    "\n",
    "      for expanded_name in expanded_names:\n",
    "        if expanded_name not in stack:\n",
    "          stack.append(expanded_name)\n",
    "\n",
    "    unreachable_ops = []\n",
    "    for op in graph.get_operations():\n",
    "      is_unreachable = False\n",
    "      all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]\n",
    "      for name in all_names:\n",
    "        if name not in seen_tensors:\n",
    "          is_unreachable = True\n",
    "      if is_unreachable:\n",
    "        unreachable_ops.append(op)\n",
    "    return unreachable_ops\n",
    "\n",
    "  @classmethod\n",
    "  def flatten_recursive(cls, item):\n",
    "    \"\"\"Flattens (potentially nested) a tuple/dictionary/list to a list.\"\"\"\n",
    "    output = []\n",
    "    if isinstance(item, list):\n",
    "      output.extend(item)\n",
    "    elif isinstance(item, tuple):\n",
    "      output.extend(list(item))\n",
    "    elif isinstance(item, dict):\n",
    "      for (_, v) in six.iteritems(item):\n",
    "        output.append(v)\n",
    "    else:\n",
    "      return [item]\n",
    "\n",
    "    flat_output = []\n",
    "    for x in output:\n",
    "      flat_output.extend(cls.flatten_recursive(x))\n",
    "    return flat_output\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.test.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
