{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import collections\n",
    "import json \n",
    "import random\n",
    "import re \n",
    "\n",
    "import modeling\n",
    "import six \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modeling\n",
    "import copy\n",
    "\n",
    "\n",
    "class BertModel(object):\n",
    "\n",
    "    \"\"\"BERT model (\"Bidirectional Encoder Representations for Transformers\").\n",
    "    \n",
    "    Example usage:\n",
    "        \n",
    "    \n",
    "        ```Python \n",
    "        # Already been converted into WordPiece token ids \n",
    "        input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
    "        input_mask = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "        input_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
    "\n",
    "        config = modeling.BertConfig(vocab_size = 32000, hidden_size = 512,\n",
    "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
    "\n",
    "        model = modeling.BertModel(config=config, is_training = True,\n",
    "        input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        label_embeddings = tf.compat.v1.get_variable(...)\n",
    "        pooled_output = model.get_pooled_output()\n",
    "        logits = tf.matmul(pooled_output, label_embeddings)\n",
    "\n",
    "        ...\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 is_training,\n",
    "                 input_ids,\n",
    "                 input_mask=None,\n",
    "                 token_type_ids=None,\n",
    "                 use_one_hot_embeddings=False,\n",
    "                 scope=None) -> None:\n",
    "        \n",
    "\n",
    "        \"\"\"Constructor for BertModel.\n",
    "        \n",
    "        Args:\n",
    "            config: `BertConfig` instance.\n",
    "            is_training: bool. True for training model, false for eval model. Controls \n",
    "                whether dropout will be applied.\n",
    "\n",
    "            input_ids: int32 Tensor of shape [batch_size, seq_len]\n",
    "            input_mask: (optional) int32 Tensor of shape [batch_size, seq_length]\n",
    "            token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length]\n",
    "            use_one_hot_embeddings: (Optional) bool. Whether to use one-hot word \n",
    "                embeddings or tf.embedding_lookup() for the word embeddings.\n",
    "\n",
    "            scope: (optional) variable scope. Defaults to \"bert\".\n",
    "\n",
    "        Raises:\n",
    "            ValueError: The config is invalid or one of the input tensor shapes\n",
    "            is invalid.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        config = copy.deepcopy(config)\n",
    "        # print(config)\n",
    "        # print(config.vocab_size)\n",
    "        if not is_training:\n",
    "            config.hidden_dropout_prob = 0.0 \n",
    "            config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "        input_shape = modeling.get_shape_list(input_ids, expected_rank=2)\n",
    "        print(\"input shape\", input_shape)\n",
    "\n",
    "        batch_size = input_shape[0]\n",
    "        seq_length = input_shape[1]\n",
    "        print(f\"Batch size: {batch_size}, sequence length: {seq_length}\")\n",
    "\n",
    "        if input_mask is None:\n",
    "            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "\n",
    "        with tf.compat.v1.variable_scope(scope, default_name=\"bert\"):\n",
    "            with tf.compat.v1.variable_scope(\"embeddings\"):\n",
    "\n",
    "                print(config.vocab_size)\n",
    "\n",
    "                # Perform embedding lookup on the word ids.\n",
    "                (self.embedding_output, self.embedding_table) = modeling.embedding_lookup(\n",
    "                    input_ids=input_ids,\n",
    "                    vocab_size=config.vocab_size,\n",
    "                    embedding_size=config.hidden_size,\n",
    "                    initializer_range=config.initializer_range,\n",
    "                    word_embedding_name=\"word_embeddings\",\n",
    "                    use_one_hot_embeddings=use_one_hot_embeddings\n",
    "                )\n",
    "\n",
    "                print(self.embedding_output)\n",
    "                print(self.embedding_table)\n",
    "\n",
    "                # Add positional embeddings and token type embeddings, then layer \n",
    "                # normalize and perform dropout.\n",
    "                \n",
    "                self.embedding_output = modeling.embedding_postprocessor(\n",
    "                    input_tensor=self.embedding_output,\n",
    "                    use_token_type=True,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    token_type_vocab_size=config.type_vocab_size,\n",
    "                    token_type_embedding_name=\"token_type_embeddings\",\n",
    "                    use_position_embeddings=True,\n",
    "                    position_embedding_name=\"position_embeddings\",\n",
    "                    initializer_range=config.initializer_range,\n",
    "                    max_position_embeddings=config.max_position_embeddings,\n",
    "                    dropout_prob=config.hidden_dropout_prob\n",
    "                )\n",
    "                \n",
    "\n",
    "                print(\"Embedding Output\", self.embedding_output)\n",
    "\n",
    "            with tf.compat.v1.variable_scope(\"encoder\"):\n",
    "\n",
    "                # This converts a 2D mask of shape [batch_size, seq_length] to a 3D \n",
    "                # mask of shape [batch_size, seq_length, seq_length] which is used \n",
    "                # for the attention scores.\n",
    "\n",
    "                attention_mask = modeling.create_attention_mask_from_input_mask(\n",
    "                    from_tensor=input_ids,\n",
    "                    to_mask=input_mask\n",
    "                )\n",
    "\n",
    "                print(\"attention_mask\", attention_mask)\n",
    "\n",
    "                # Run the stocked transformer.\n",
    "                # `Sequence_output` shape = [batch_size, seq_length, hidden_size]\n",
    "                \n",
    "                self.all_encoder_layers = modeling.transformer_model(\n",
    "                input_tensor=self.embedding_output,\n",
    "                attention_mask=attention_mask,\n",
    "                hidden_size=config.hidden_size,\n",
    "                num_hidden_layers=config.num_hidden_layers,\n",
    "                num_attention_heads=config.num_attention_heads,\n",
    "                intermediate_size=config.intermediate_size,\n",
    "                intermediate_act_fn= modeling.get_activation(config.hidden_act),\n",
    "                hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "                attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "                initializer_range=config.initializer_range,\n",
    "                do_return_all_layers=True)\n",
    "\n",
    "\n",
    "            self.sequence_output = self.all_encoder_layers[-1]\n",
    "\n",
    "            # The \"pooler\" converts the encoded sequence tensor of shape \n",
    "            # [batch_size, seq_length, hidden_size] to a tensor of shape \n",
    "            # [batch_size, hidden_size]. This is neccessary for segment-level \n",
    "            # (or segment-pair-level) classification task where we need a fixed \n",
    "            # dimensional representation of the segment.\n",
    "\n",
    "            with tf.compat.v1.variable_scope(\"pooler\"):\n",
    "\n",
    "                # we \"pool\" the model by simply taking the hidden state corresponding \n",
    "                # to the first token. we assume that this has been pre-trained.\n",
    "\n",
    "                first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
    "                self.pooled_output = tf.keras.layers.Dense(\n",
    "                    units=config.hidden_size,\n",
    "                    activation=tf.tanh,\n",
    "                    kernel_initializer= modeling.create_initializer(config.initializer_range)\n",
    "                )(first_token_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_pooled_output(self):\n",
    "        return self.pooled_output\n",
    "    \n",
    "\n",
    "    def get_sequence_output(self):\n",
    "        \"\"\"Gets final hidden layer of encoder.\n",
    "        \n",
    "\n",
    "        Returns:\n",
    "            float Tensor of shape [batch_size, seq_length, hidden_size] corresponding \n",
    "            to the final hidden of the transformer encoder.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        return self.sequence_output\n",
    "    \n",
    "\n",
    "    def get_all_encoder_layers(self):\n",
    "        return self.all_encoder_layers\n",
    "    \n",
    "\n",
    "    def get_embedding_output(self):\n",
    "\n",
    "        \"\"\"Gets output of the embedding lookup (i.e, input to the transformer.)\n",
    "        \n",
    "        Returns:\n",
    "            float Tensor of shape [batch_sie, seq_length, hidden_size] correspoinding \n",
    "            to the output of the embedding layer, after summing the word \n",
    "            embedding with the positional embeddings and the token type embedings.\n",
    "            the performing layer normalization. This is the input to the transformer.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        return self.embedding_output\n",
    "    \n",
    "\n",
    "\n",
    "    def get_embedding_table(self):\n",
    "        return self.embedding_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape [2, 3]\n",
      "Batch size: 2, sequence length: 3\n",
      "32000\n",
      "tf.Tensor(\n",
      "[[[-0.03295476 -0.01691835  0.00876137 ... -0.0124014  -0.01919353\n",
      "   -0.01375115]\n",
      "  [-0.01643722 -0.0030208  -0.00078725 ... -0.02569843  0.00738917\n",
      "    0.01281514]\n",
      "  [-0.00244974  0.01686387  0.01088041 ... -0.0246888  -0.00314107\n",
      "    0.00057836]]\n",
      "\n",
      " [[ 0.00209959 -0.0097736  -0.00558968 ... -0.00036225  0.01715203\n",
      "    0.00660457]\n",
      "  [ 0.01441314 -0.00921718 -0.01999765 ...  0.00740988 -0.01414874\n",
      "    0.0012064 ]\n",
      "  [-0.00656144  0.00375049 -0.00575585 ... -0.01238892 -0.03380669\n",
      "    0.00518762]]], shape=(2, 3, 512), dtype=float32)\n",
      "<tf.Variable 'bert_1/embeddings/word_embeddings:0' shape=(32000, 512) dtype=float32, numpy=\n",
      "array([[-0.00656144,  0.00375049, -0.00575585, ..., -0.01238892,\n",
      "        -0.03380669,  0.00518762],\n",
      "       [ 0.02966835,  0.01244527, -0.00564474, ...,  0.02363148,\n",
      "        -0.00556675,  0.02400868],\n",
      "       [ 0.01303717, -0.01035753,  0.01226201, ..., -0.02942312,\n",
      "         0.03174744,  0.00731598],\n",
      "       ...,\n",
      "       [-0.00998403,  0.0144923 ,  0.01929289, ..., -0.00358558,\n",
      "        -0.01358786,  0.02916626],\n",
      "       [-0.0138947 , -0.0138718 ,  0.00552165, ...,  0.0001751 ,\n",
      "        -0.00349083,  0.00788197],\n",
      "       [ 0.00381802,  0.01990628, -0.02694307, ..., -0.0047733 ,\n",
      "         0.01368865,  0.0157438 ]], dtype=float32)>\n",
      "Embedding Output tf.Tensor(\n",
      "[[[ 0.         0.         1.7543333 ...  0.         0.         0.       ]\n",
      "  [ 0.         0.        -7.4315615 ...  0.         0.         0.       ]\n",
      "  [ 0.         0.         0.        ...  0.         0.         0.       ]]\n",
      "\n",
      " [[ 0.         0.        -2.035669  ...  0.         0.         0.       ]\n",
      "  [ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      "  [ 0.         0.         0.        ...  0.         0.        -5.1441917]]], shape=(2, 3, 512), dtype=float32)\n",
      "attention_mask tf.Tensor(\n",
      "[[[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]], shape=(2, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
    "\n",
    "config = modeling.BertConfig(vocab_size=32000, hidden_size=512, num_hidden_layers=8, num_attention_heads=8, intermediate_size=1024)\n",
    "model = BertModel(config=config, is_training=True, input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tests under Python 3.12.0: c:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n",
      "[ RUN      ] BertModelTest.test_create_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids Tensor(\"random_uniform:0\", shape=(13, 7), dtype=int32)\n",
      "INFO:tensorflow:time(__main__.BertModelTest.test_create_model): 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1006 18:44:36.937553 14964 test_util.py:2634] time(__main__.BertModelTest.test_create_model): 0.0s\n",
      "[       OK ] BertModelTest.test_create_model\n",
      "[ RUN      ] BertModelTest.test_session\n",
      "[  SKIPPED ] BertModelTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.003s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manjusha Kumari\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from absl import flags\n",
    "\n",
    "# Define a dummy flag to avoid the 'unknown command line flag' error \n",
    "# flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "class BertModelTest(tf.test.TestCase):\n",
    "\n",
    "    class BertModelTester(object):\n",
    "\n",
    "        def __init__(self,\n",
    "                     parent,\n",
    "                     batch_size=13,\n",
    "                     seq_length=7,\n",
    "                     is_training=True,\n",
    "                     use_input_mask=True,\n",
    "                     use_token_type_ids=True,\n",
    "                     vocab_size=99,\n",
    "                     hidden_size=32,\n",
    "                     num_hidden_layers=5,\n",
    "                     num_attention_heads=4,\n",
    "                     intermediate_size=37,\n",
    "                     hidden_act=\"gelu\",\n",
    "                     hidden_dropout_prob=0.1,\n",
    "                     attention_probs_dropout_prob=0.1,\n",
    "                     max_position_embeddings=512,\n",
    "                     type_vocab_size=16,\n",
    "                     initializer_range=0.02,\n",
    "                     scope=None) -> None:\n",
    "            \n",
    "            self.parent = parent\n",
    "            self.batch_size = batch_size\n",
    "            self.seq_length = seq_length\n",
    "            self.is_training = is_training\n",
    "            self.use_input_mask = use_input_mask\n",
    "            self.use_token_type_ids = use_token_type_ids\n",
    "            self.vocab_size = vocab_size\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_hidden_layers = num_hidden_layers\n",
    "            self.num_attention_heads = num_attention_heads\n",
    "            self.intermediate_size = intermediate_size\n",
    "            self.hidden_act = hidden_act\n",
    "            self.hidden_dropout_prob = hidden_dropout_prob\n",
    "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "            self.max_position_embeddings = max_position_embeddings\n",
    "            self.type_vocab_size = type_vocab_size\n",
    "            self.initializer_range = initializer_range\n",
    "            self.scope = scope\n",
    "\n",
    "        def create_model(self):\n",
    "            input_ids = BertModelTest.ids_tensor([self.batch_size, self.seq_length],\n",
    "                                                 self.vocab_size)\n",
    "            print(\"input_ids\", input_ids)\n",
    "\n",
    "            # input_mask = None \n",
    "            # if self.use_input_mask:\n",
    "            #     input_mask = BertModelTest.ids_tensor(\n",
    "            #         [self.batch_size, self.seq_length], vocab_size=2\n",
    "            #     )\n",
    "\n",
    "            #     print(\"input_mask\", input_mask)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def ids_tensor(shape, vocab_size):\n",
    "        \"\"\"Creates a random tensor of the given shape and vocab size.\"\"\"\n",
    "        return tf.random.uniform(shape, maxval=vocab_size, dtype=tf.int32)\n",
    "\n",
    "    def test_create_model(self):\n",
    "        tester = self.BertModelTester(parent=self)\n",
    "        tester.create_model()\n",
    "\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.test.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "import six\n",
    "\n",
    "class Flattener:\n",
    "    @classmethod\n",
    "    def flatten_recursive(cls, item):\n",
    "        \"\"\"Flattens (potentially nested) a tuple/dictionary/list to a list.\"\"\"\n",
    "        output = []\n",
    "        if isinstance(item, list):\n",
    "            output.extend(item)\n",
    "        elif isinstance(item, tuple):\n",
    "            output.extend(list(item))\n",
    "        elif isinstance(item, dict):\n",
    "            for v in item.values():\n",
    "                output.append(v)\n",
    "        else:\n",
    "            return [item]\n",
    "\n",
    "        flat_output = []\n",
    "        for x in output:\n",
    "            flat_output.extend(cls.flatten_recursive(x))\n",
    "        return flat_output\n",
    "\n",
    "# Example usage:\n",
    "nested_structure = [1, (2, 3), {'a': 4, 'b': [5, 6]}, 7]\n",
    "flattened = Flattener.flatten_recursive(nested_structure)\n",
    "print(flattened)  # Output: [1, 2, 3, 4, 5, 6, 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_unreachable_ops(cls, graph, outputs):\n",
    "\n",
    "    \"\"\"Finds all of the tensors in graph that are unreachable from outputs.\"\"\"\n",
    "\n",
    "    outputs = cls.flatten_recursive(outputs)\n",
    "    print(\"outputs\", outputs)\n",
    "    output_to_op = collections.defaultdict(list)\n",
    "    print(\"output_to_op\", output_to_op)\n",
    "    op_to_all = collections.defaultdict(list)\n",
    "    assign_out_to_in = collections.defaultdict(list)\n",
    "\n",
    "    for op in graph.get_operations():\n",
    "        print(\"op\", op)\n",
    "        for x in op.inputs:\n",
    "            print(\"x: \", x)\n",
    "            op_to_all[op.name].append(x.name)\n",
    "            print(\"op_to_all: \", op_to_all)\n",
    "\n",
    "        for y in op.outputs:\n",
    "            print(\"y\", y)\n",
    "            output_to_op[y.name].append(op.name)\n",
    "            print(\"output_to_op: %s\" % output_to_op)\n",
    "\n",
    "            op_to_all[op.name].append(y.name)\n",
    "            print(\"op_to_all %s\" % op_to_all)\n",
    "\n",
    "        if str(op.type) == \"Assign\":\n",
    "            print(\"match the assign\")\n",
    "            for y in op.outputs:\n",
    "                for x in op.inputs:\n",
    "                    assign_out_to_in[y.name].append(x.name)\n",
    "                    print(\"assign_out_to_in %s\" % assign_out_to_in)\n",
    "\n",
    "\n",
    "\n",
    "    assign_groups = collections.defaultdict(list)\n",
    "    for out_name in assign_out_to_in.keys():\n",
    "        print(\"out_name: \", out_name)\n",
    "\n",
    "        name_group = assign_out_to_in[out_name]\n",
    "        for n1 in name_group:\n",
    "            assign_groups[n1].append(out_name)\n",
    "\n",
    "            for n2 in name_group:\n",
    "                if n1 != n2:\n",
    "                    assign_groups[n1].append(n2)\n",
    "                    print(\"assign_group\", assign_groups)\n",
    "\n",
    "\n",
    "    seen_tensors = {}\n",
    "    stack = [x.name for x in outputs]\n",
    "    print(\"stack\", stack)\n",
    "    while stack:\n",
    "        name = stack.pop()\n",
    "        print(\"name\", name)\n",
    "\n",
    "        if name in seen_tensors:\n",
    "            continue\n",
    "\n",
    "        seen_tensors[name] = True\n",
    "        print(\"seen_tensors: %s\" % seen_tensors)\n",
    "\n",
    "        if name in output_to_op:\n",
    "            for op_name in output_to_op[name]:\n",
    "                print(\"op_name\", op_name)\n",
    "\n",
    "                if op_name in op_to_all:\n",
    "                    for input_name in op_to_all[op.name]:\n",
    "                        print(\"input_name\", input_name)\n",
    "\n",
    "                        if input_name not in stack:\n",
    "                            stack.append(input_name)\n",
    "                            print(\"stack\", stack)\n",
    "\n",
    "\n",
    "\n",
    "        expended_names = []\n",
    "        if name in assign_groups:\n",
    "            print(\"name\", name)\n",
    "            \n",
    "            for assign_name in assign_groups[name]:\n",
    "                print(\"assign_name\", assign_name)\n",
    "                expended_names.append(assign_name)\n",
    "\n",
    "\n",
    "        for expended_name in expended_names:\n",
    "            print(\"expended name\", expended_name)\n",
    "\n",
    "            if expended_name not in stack:\n",
    "                stack.append(expended_name)\n",
    "\n",
    "                print(\"stack\", stack)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    unreachable_ops = []\n",
    "    for op in graph.get_operations():\n",
    "        is_unreachable = False \n",
    "        \n",
    "        print(\"op: \", op)\n",
    "\n",
    "        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]\n",
    "        print(\"all_names\", all_names)\n",
    "\n",
    "\n",
    "        for name in all_names:\n",
    "            if name not in seen_tensors:\n",
    "                is_unreachable = True\n",
    "\n",
    "        if is_unreachable:\n",
    "            unreachable_ops.append(op)\n",
    "            print(\"unreachable_ops\", unreachable_ops)\n",
    "\n",
    "\n",
    "    return unreachable_ops\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a simple graph \n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    a = tf.constant(1, name='a')\n",
    "    b = tf.constant(2, name='b')\n",
    "    c = tf.add(a, b, name='c')\n",
    "    d = tf.multiply(c, b, name='d')\n",
    "\n",
    "\n",
    "# Example usage\n",
    "outputs = [d]\n",
    "unreachable_ops = get_unreachable_ops(cls=Flattener, graph=graph, outputs=outputs)\n",
    "# print(\"Unreachable operations:\", [op.name for op in unreachable_ops])\n",
    "unreachable_ops\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# stack =  ['d:0']\n",
    "this_is_list = {}\n",
    "stack =  ['we']\n",
    "a = stack.pop()\n",
    "b = this_is_list[a] = True \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unreachable []\n",
      "Unreachable ops: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def assert_all_tensors_reachable(sess, outputs):\n",
    "    \"\"\"Checks that all the tensors in the graph are reachable from outputs.\"\"\"\n",
    "    graph = sess.graph\n",
    "\n",
    "    ignore_strings = [\n",
    "        \"^.*/assert_less_equal/.*$\",\n",
    "        \"^.*/dilation_rate$\",\n",
    "        \"^.*/Tensordot/concat$\",\n",
    "        \"^.*/Tensordot/concat/axis$\",\n",
    "        \"^testing/.*$\",\n",
    "    ]\n",
    "\n",
    "    ignore_regexes = [re.compile(x) for x in ignore_strings]\n",
    "\n",
    "    unreachable = get_unreachable_ops(cls=None, graph=graph, outputs=outputs)\n",
    "    print(\"unreachable\", unreachable)\n",
    "\n",
    "    filtered_unreacable = []\n",
    "\n",
    "    for x in unreachable:\n",
    "        do_ignore = False\n",
    "\n",
    "        for r in ignore_regexes:\n",
    "            m = r.match(x.name)\n",
    "            if m is not None:\n",
    "                do_ignore = True\n",
    "\n",
    "        if do_ignore:\n",
    "            continue\n",
    "\n",
    "        filtered_unreacable.append(x)\n",
    "        print(\"filtered_unreachable\", filtered_unreacable)\n",
    "\n",
    "\n",
    "    unreachable = filtered_unreacable\n",
    "\n",
    "    # Debugging print statment \n",
    "    print(\"Unreachable ops:\", [x.name for x in unreachable])\n",
    "\n",
    "    self.assertEqual(\n",
    "        len(unreachable), 0 , \"the followeing ops are unreachable: %s\" % \n",
    "        (\" \".join([x.name for x in unreachable]))\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a simple graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    a = tf.constant(2, name='a')\n",
    "    b = tf.constant(3, name='b')\n",
    "    c = tf.add(a, b, name='c')\n",
    "\n",
    "# Create a session and run the assert function\n",
    "with tf.compat.v1.Session(graph=graph) as sess:\n",
    "    outputs = [c]\n",
    "    assert_all_tensors_reachable(sess, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "def ids_tensor(cls, shape, vocab_size, rng=None, name=None):\n",
    "    \"\"\"Creates a random int32 tensor of the shape within the vocab size.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = random.Random()\n",
    "\n",
    "    total_dims = 1\n",
    "    for dim in shape:\n",
    "        total_dims *= dim \n",
    "\n",
    "    values = [rng.randint(0, vocab_size - 1) for _ in range(total_dims)]\n",
    "\n",
    "    return tf.constant(value=values, dtype=tf.int32, shape=shape, name=name)\n",
    "\n",
    "# Example usage\n",
    "class DummyClass:\n",
    "    pass\n",
    "\n",
    "shape = (2, 3)\n",
    "vocab_size = 10\n",
    "tensor = ids_tensor(DummyClass, shape, vocab_size)\n",
    "\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
