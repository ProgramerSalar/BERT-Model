{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import re \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWeightDecayOptimizer(tf.compat.v1.train.Optimizer):\n",
    "\n",
    "    \"\"\"A basic Adam optimizer that include \"correct\" L2 weight decay. \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate,\n",
    "                 weight_decay_rate=0.0,\n",
    "                 beta_1=0.9,\n",
    "                 beta_2=0.999,\n",
    "                 epsilon=1e-6,\n",
    "                 exclude_from_weight_decay=None,\n",
    "                 name=\"AdamWeightDecayOptimizer\",\n",
    "                 ):\n",
    "        \n",
    "\n",
    "        \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
    "        super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay_rate = weight_decay_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2 \n",
    "        self.epsilon = epsilon \n",
    "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "\n",
    "\n",
    "    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
    "\n",
    "        \"\"\"See base class.\"\"\"\n",
    "\n",
    "        assignment = []\n",
    "        for (grad, param) in grads_and_vars:\n",
    "            if grad is None or param is None:\n",
    "                continue\n",
    "\n",
    "\n",
    "            param_name = self._get_variable_name(param.name)\n",
    "\n",
    "            m = tf.compat.v1.get_variable(\n",
    "                name=param_name + \"/adam_m\",\n",
    "                shape = param.shape.as_list(),\n",
    "                dtype=tf.float32,\n",
    "                trainable=False,\n",
    "                initializer=tf.zeros_initializer()\n",
    "            )\n",
    "\n",
    "            v = tf.get_variable(\n",
    "                name = param_name + \"/adam_v\",\n",
    "                shape = param.shape.as_list()\n",
    "                dtype = tf.float32,\n",
    "                trainable = False,\n",
    "                initializer = tf.zero_initializer()\n",
    "            )\n",
    "\n",
    "\n",
    "            # Standard Adam update.\n",
    "            next_m = (\n",
    "                tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad)\n",
    "            )\n",
    "\n",
    "            next_v = tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2, tf.square(grad))\n",
    "\n",
    "            update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
    "\n",
    "\n",
    "            # Just adding the square of the weight to the loss function is *not*\n",
    "            # the correct way of using L2 regularization/weight decay with Adam,\n",
    "            # since that will interact with the m and v parameters in strage ways.\n",
    "\n",
    "            # Instead we want to decay the weight in a manner that doesn't iteract \n",
    "            # with the m/v parameters. This is equivalent to adding the square \n",
    "            # of the weight to the loss with plain (non-momentum) SGD.\n",
    "\n",
    "            if self._do_use_weight_decay(param_name):\n",
    "                update += self.weight_decay_rate * param\n",
    "\n",
    "            update_with_lr = self.learning_rate * update\n",
    "\n",
    "            next_param = param - update_with_lr\n",
    "\n",
    "            assignment.extend(\n",
    "                [param.assign(next_param),\n",
    "                 m.assign(next_m),\n",
    "                 v.assign(next_v)\n",
    "                 ])\n",
    "            \n",
    "            return tf.group(*assignment, name=name)\n",
    "        \n",
    "\n",
    "    def _do_use_weight_decay(self, param_name):\n",
    "\n",
    "        \"\"\"Whether to use L2 weight decay for `param_name`. \"\"\"\n",
    "\n",
    "        if not self.weight_decay_rate:\n",
    "            return False\n",
    "        \n",
    "        if self.exclude_from_weight_decay:\n",
    "            for r in self.exclude_from_weight_decay:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "                \n",
    "        return True\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def _get_variable_name(self, param_name):\n",
    "\n",
    "        \"\"\"GEt the variable name from the tensor name.\"\"\"\n",
    "        m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
    "        if m is not None:\n",
    "            param_name = m.group(1)\n",
    "        return param_name\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_step, use_tpu):\n",
    "\n",
    "    \"\"\"Creates an optimizer training op.\"\"\"\n",
    "\n",
    "    global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "    learning_rate = tf.constant(value=init_lr,\n",
    "                                shape=[],\n",
    "                                dtype=tf.float32)\n",
    "    \n",
    "\n",
    "    # Implements linear decay of the learning rate.\n",
    "    learning_rate = tf.compat.v1.train.polynomial_decay(\n",
    "        learning_rate,\n",
    "        global_step,\n",
    "        num_train_steps,\n",
    "        end_learning_rate=0.0,\n",
    "        power=1.0,\n",
    "        cycle=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Implements linear warmup. I.e, if global_step < num_warmup_step, \n",
    "    # the learning rate will be `global_step/num_warmup_step * init_lr`.\n",
    "\n",
    "    if num_warmup_step:\n",
    "        global_step_int = tf.cast(global_step, tf.int32)\n",
    "        warmup_steps_int = tf.concat(num_warmup_step, dtype=tf.int32)\n",
    "\n",
    "        global_steps_float = tf.cast(global_step_int, tf.float32)\n",
    "        warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "\n",
    "        warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "        warmup_learning_rate = init_lr * warmup_percent_done\n",
    "\n",
    "\n",
    "        is_warmup = tf.cast(global_step_int < warmup_steps_float, tf.float32)\n",
    "        learning_rate = (\n",
    "            (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate\n",
    "        )\n",
    "\n",
    "\n",
    "        # It is recommended that you use this optimizer for fine tuning, since this \n",
    "        # is how the model was trained (note that the Adam m/v variable are Not loaded from init_checkpoint.)\n",
    "\n",
    "\n",
    "        optimizer = AdamWeightDecayOptimizer(\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay_rate=0.01,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-6,\n",
    "            exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"]\n",
    "        )\n",
    "\n",
    "\n",
    "        if use_tpu:\n",
    "            optimizer = tf.compat.v1.tpu.CrossShardOptimizer(opt=optimizer)\n",
    "\n",
    "        tvars = tf.compat.v1.trainable_variables()\n",
    "        grads = tf.compat.v1.gradients(loss, tvars)\n",
    "\n",
    "\n",
    "        # This is how the model was pre-trained.\n",
    "        (grad, _) = tf.compat.v1.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step=global_step\n",
    "        )\n",
    "\n",
    "\n",
    "        # Normally the global step update is done inside of `apply_gradients`.\n",
    "        # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use \n",
    "        # a different optimizer, you should probably take this line out.\n",
    "\n",
    "        new_global_step = global_step + 1\n",
    "        train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
    "        return train_op\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
